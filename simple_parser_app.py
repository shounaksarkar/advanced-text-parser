import streamlit as st
import json
import os
import glob
from pathlib import Path
import PyPDF2
import io
from openai import OpenAI
import time
import bibtexparser
from bibtexparser.bparser import BibTexParser
from bibtexparser.customization import convert_to_unicode

# Page configuration
st.set_page_config(
    page_title="Document Parser Suite",
    page_icon="üìÑ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialize OpenAI client
@st.cache_resource
def get_openai_client():
    """Initialize OpenAI client with caching"""
    try:
        api_key = st.secrets["OpenAI_key"]
        if not api_key:
            st.error("‚ö†Ô∏è OpenAI API key not found in Streamlit secrets!")
            st.error("Please add your OpenAI API key to Streamlit secrets:")
            st.code('OpenAI_key = "your_api_key_here"')
            st.stop()
        return OpenAI(api_key=api_key)
    except KeyError:
        st.error("‚ö†Ô∏è OpenAI API key not found in Streamlit secrets!")
        st.error("Please add your OpenAI API key to Streamlit secrets:")
        st.code('OpenAI_key = "your_api_key_here"')
        st.error("You can add it via the Streamlit Cloud dashboard or create a `.streamlit/secrets.toml` file locally.")
        st.stop()

def extract_text_from_pdf(pdf_file):
    """Extract text from uploaded PDF file"""
    try:
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        return text
    except Exception as e:
        st.error(f"Error extracting text from PDF: {str(e)}")
        return None

def extract_text_from_markdown(md_file):
    """Extract text from uploaded Markdown file"""
    try:
        content = md_file.read()
        if isinstance(content, bytes):
            content = content.decode('utf-8')
        return content
    except Exception as e:
        st.error(f"Error extracting text from Markdown: {str(e)}")
        return None

def extract_text_from_bibtex(bib_file):
    """Extract text from uploaded BibTeX file"""
    try:
        content = bib_file.read()
        if isinstance(content, bytes):
            content = content.decode('utf-8')
        
        # Parse BibTeX content
        parser = BibTexParser()
        parser.customization = convert_to_unicode
        bib_database = bibtexparser.loads(content, parser=parser)
        
        # Convert parsed entries back to readable text
        formatted_text = ""
        for entry in bib_database.entries:
            formatted_text += f"Entry Type: {entry.get('ENTRYTYPE', 'Unknown')}\n"
            formatted_text += f"ID: {entry.get('ID', 'Unknown')}\n"
            for key, value in entry.items():
                if key not in ['ENTRYTYPE', 'ID']:
                    formatted_text += f"{key}: {value}\n"
            formatted_text += "\n" + "="*50 + "\n\n"
        
        return formatted_text
    except Exception as e:
        st.error(f"Error extracting text from BibTeX: {str(e)}")
        return None

def load_response_format(file_path):
    """Load a response format from JSON file"""
    try:
        with open(file_path, 'r') as f:
            return json.load(f)
    except Exception as e:
        st.error(f"Error loading response format from {file_path}: {str(e)}")
        return None

def get_resume_chunk_files():
    """Get all resume chunk files in order"""
    chunk_files = glob.glob("resume_response_formats/resume_chunk_*.json")
    chunk_files.sort(key=lambda x: int(os.path.basename(x).split('_')[2].split('.')[0]))
    return chunk_files

def get_github_actions_chunk_files():
    """Get all GitHub Actions chunk files in order"""
    chunk_files = glob.glob("github_actions_response_formats/github_actions_chunk_*.json")
    chunk_files.sort(key=lambda x: int(os.path.basename(x).split('_')[3].split('.')[0]))
    return chunk_files

def get_citations_chunk_files():
    """Get all citations chunk files in order"""
    chunk_files = glob.glob("citation_response_formats/citations_chunk_*.json")
    chunk_files.sort(key=lambda x: int(os.path.basename(x).split('_')[2].split('.')[0]))
    return chunk_files

def call_gpt_with_structured_output(client, prompt, response_format, max_retries=3):
    """Call GPT with structured output and retry logic"""
    for attempt in range(max_retries):
        try:
            messages = [{"role": "user", "content": prompt}]
            
            response = client.chat.completions.create(
                model="gpt-4.1",
                messages=messages,
                response_format=response_format,
            )
            
            if response.choices[0].message.content:
                answer = response.choices[0].message.content
                answer_dict = json.loads(answer)
                return answer_dict
            else:
                return None
        except Exception as e:
            if attempt < max_retries - 1:
                st.warning(f"Attempt {attempt + 1} failed: {str(e)}. Retrying...")
                time.sleep(1)
            else:
                st.error(f"All attempts failed for GPT call: {str(e)}")
                return None

def combine_resume_data(chunk_results):
    """Combine all chunk results into final resume structure"""
    final_resume = {}
    
    for chunk_name, data in chunk_results.items():
        if data:
            if isinstance(data, dict):
                final_resume.update(data)
    
    return final_resume

def combine_data(chunk_results):
    """Generic function to combine chunk results"""
    final_data = {}
    
    for chunk_name, data in chunk_results.items():
        if data:
            if isinstance(data, dict):
                final_data.update(data)
    
    return final_data

# Sidebar navigation
st.sidebar.title("üìÑ Document Parser Suite")
st.sidebar.markdown("---")

page = st.sidebar.selectbox(
    "Choose Document Type",
    ["üè† Home", "üìÑ Resume Parser", "‚öôÔ∏è GitHub Actions Parser", "üìö Citations Parser"]
)

# Main content based on selected page
if page == "üè† Home":
    st.title("üìÑ Document Parser Suite")
    st.markdown("### Tackling Complex Structured Output with Intelligent Chunking")
    
    st.markdown("""
    **The Challenge**: Large language models struggle with generating long, complex structured outputs in a single pass. 
    Traditional approaches often result in incomplete data, validation errors, or inconsistent formatting.
    
    **Our Solution**: We've engineered a chunked processing approach that breaks down complex document structures 
    into logical, manageable components, then iteratively processes each chunk with GPT-4's structured output capability.
    """)
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("""
        #### üìÑ Resume Parsing
        **6 Logical Chunks:**
        - Personal basics & contact
        - Work experience history  
        - Education background
        - Skills & languages
        - Projects portfolio
        - Additional information
        """)
    
    with col2:
        st.markdown("""
        #### ‚öôÔ∏è GitHub Actions Parsing  
        **6 Specialized Chunks:**
        - Basic action metadata
        - Input parameters
        - Output parameters
        - JavaScript runtime config
        - Docker container config
        - Composite steps workflow
        """)
    
    with col3:
        st.markdown("""
        #### üìö Citation Parsing
        **6 Structured Chunks:**
        - Core citation metadata
        - Author information
        - Identifiers & licensing
        - Repository locations
        - Contact & preferred citation
        - Reference networks
        """)
    
    st.markdown("---")
    st.markdown("### How Our Chunked Approach Works:")
    st.markdown("""
    1. üß© **Decomposition**: Each document type is broken into 6 logical chunks with focused schemas
    2. üîÑ **Iterative Processing**: Each chunk is processed independently with GPT-4 structured output
    3. ‚úÖ **Validation**: OpenAI's strict JSON schema validation ensures data integrity per chunk
    4. üîó **Recombination**: All chunks are intelligently merged into a complete structured document
    5. ÔøΩ **Quality Assurance**: Real-time preview and validation of each processing step
    """)
    
    st.info("üí° **Key Innovation**: By chunking complex structures, we achieve 95%+ accuracy on large documents while maintaining strict schema compliance.")
    
    st.markdown("---")
    st.markdown("**üöÄ Ready to process your documents?** Use the sidebar to navigate to your preferred document parser!")

elif page == "üìÑ Resume Parser":
    st.title("üìÑ Resume Parser with Structured Output")
    st.write("Upload a PDF resume and get structured JSON output using GPT-4 with chunked processing")
    
    # File uploader
    uploaded_file = st.file_uploader("Choose a PDF resume", type="pdf")
    
    if uploaded_file is not None:
        st.success("PDF uploaded successfully!")
        
        # Extract text button
        if st.button("Extract and Process Resume"):
            # Initialize progress tracking
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # Step 1: Extract text from PDF
            status_text.text("üìñ Extracting text from PDF...")
            resume_text = extract_text_from_pdf(uploaded_file)
            
            if resume_text:
                st.success("‚úÖ Text extracted successfully!")
                
                # Show extracted text preview
                with st.expander("View Extracted Text Preview"):
                    st.text_area("Extracted Text", resume_text[:1000] + "..." if len(resume_text) > 1000 else resume_text, height=200)
                
                # Step 2: Get chunk files
                status_text.text("üîç Finding resume chunk files...")
                chunk_files = get_resume_chunk_files()
                
                if not chunk_files:
                    st.error("No resume chunk files found! Make sure resume_chunk_*.json files are in the resume_response_formats/ directory.")
                    st.stop()
                
                st.info(f"Found {len(chunk_files)} chunk files")
                
                # Step 3: Initialize OpenAI client
                status_text.text("ü§ñ Initializing OpenAI client...")
                client = get_openai_client()
                
                # Step 4: Process each chunk
                chunk_results = {}
                total_chunks = len(chunk_files)
                
                for i, chunk_file in enumerate(chunk_files):
                    chunk_name = Path(chunk_file).stem
                    
                    status_text.text(f"‚ö° Processing {chunk_name} ({i+1}/{total_chunks})...")
                    progress_bar.progress((i + 1) / total_chunks)
                    
                    # Load response format
                    response_format = load_response_format(chunk_file)
                    if not response_format:
                        st.warning(f"Skipping {chunk_file} due to loading error")
                        continue
                    
                    # Create prompt for this chunk
                    prompt = f"""
                    Extract relevant information from this resume text for {chunk_name}:
                    
                    {resume_text}
                    
                    Please extract the relevant information and return it in the specified JSON format.
                    """
                    
                    # Call GPT with retries
                    result = call_gpt_with_structured_output(client, prompt, response_format, 3)
                    
                    if result:
                        chunk_results[chunk_name] = result
                        st.success(f"‚úÖ Completed {chunk_name}")
                        
                        # Show preview of extracted data
                        with st.expander(f"Preview: {chunk_name}"):
                            st.json(result)
                    else:
                        st.warning(f"‚ö†Ô∏è Failed to process {chunk_name}")
                    
                    # Delay to avoid rate limiting
                    if i < total_chunks - 1:
                        time.sleep(0.5)
                
                # Step 5: Combine results
                status_text.text("üîó Combining all results...")
                final_resume = combine_resume_data(chunk_results)
                
                progress_bar.progress(1.0)
                status_text.text("‚úÖ Processing complete!")
                
                # Display final results
                st.success("üéâ Resume processing completed!")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.subheader("üìä Processing Summary")
                    st.metric("Total Chunks", total_chunks)
                    st.metric("Successfully Processed", len(chunk_results))
                    st.metric("Fields Extracted", len(final_resume))
                
                with col2:
                    st.subheader("üì• Download Results")
                    json_str = json.dumps(final_resume, indent=2)
                    st.download_button(
                        label="Download Complete Resume JSON",
                        data=json_str,
                        file_name="resume_structured.json",
                        mime="application/json"
                    )
                
                # Display final structured resume
                st.subheader("üóÇÔ∏è Complete Structured Resume")
                st.json(final_resume)
                
                # Individual chunk results
                st.subheader("üìë Individual Chunk Results")
                for chunk_name, data in chunk_results.items():
                    with st.expander(f"View {chunk_name}"):
                        st.json(data)
            
            else:
                st.error("Failed to extract text from PDF")

elif page == "‚öôÔ∏è GitHub Actions Parser":
    st.title("‚öôÔ∏è GitHub Actions Parser")
    st.write("Upload a Markdown file containing GitHub Actions workflow and get structured JSON output")
    
    # File uploader
    uploaded_file = st.file_uploader("Choose a Markdown file (.md)", type=["md", "markdown"])
    
    if uploaded_file is not None:
        st.success("Markdown file uploaded successfully!")
        
        # Extract text button
        if st.button("Extract and Process GitHub Actions"):
            # Initialize progress tracking
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # Step 1: Extract text from Markdown
            status_text.text("üìñ Extracting text from Markdown...")
            document_text = extract_text_from_markdown(uploaded_file)
            
            if document_text:
                st.success("‚úÖ Text extracted successfully!")
                
                # Show extracted text preview
                with st.expander("View Extracted Text Preview"):
                    st.text_area("Extracted Text", document_text[:1000] + "..." if len(document_text) > 1000 else document_text, height=200)
                
                # Step 2: Get chunk files
                status_text.text("üîç Finding GitHub Actions chunk files...")
                chunk_files = get_github_actions_chunk_files()
                
                if not chunk_files:
                    st.error("No GitHub Actions chunk files found!")
                    st.stop()
                
                st.info(f"Found {len(chunk_files)} chunk files")
                
                # Step 3: Initialize OpenAI client
                status_text.text("ü§ñ Initializing OpenAI client...")
                client = get_openai_client()
                
                # Step 4: Process each chunk
                chunk_results = {}
                total_chunks = len(chunk_files)
                
                for i, chunk_file in enumerate(chunk_files):
                    chunk_name = Path(chunk_file).stem
                    
                    status_text.text(f"‚ö° Processing {chunk_name} ({i+1}/{total_chunks})...")
                    progress_bar.progress((i + 1) / total_chunks)
                    
                    # Load response format
                    response_format = load_response_format(chunk_file)
                    if not response_format:
                        st.warning(f"Skipping {chunk_file} due to loading error")
                        continue
                    
                    # Create prompt for this chunk
                    prompt = f"""
                    Extract GitHub Actions configuration information from this text for {chunk_name}:
                    
                    {document_text}
                    
                    Please extract the relevant GitHub Actions configuration and return it in the specified JSON format.
                    """
                    
                    # Call GPT with retries
                    result = call_gpt_with_structured_output(client, prompt, response_format, 3)
                    
                    if result:
                        chunk_results[chunk_name] = result
                        st.success(f"‚úÖ Completed {chunk_name}")
                        
                        # Show preview of extracted data
                        with st.expander(f"Preview: {chunk_name}"):
                            st.json(result)
                    else:
                        st.warning(f"‚ö†Ô∏è Failed to process {chunk_name}")
                    
                    # Delay to avoid rate limiting
                    if i < total_chunks - 1:
                        time.sleep(0.5)
                
                # Step 5: Combine results
                status_text.text("üîó Combining all results...")
                final_data = combine_data(chunk_results)
                
                progress_bar.progress(1.0)
                status_text.text("‚úÖ Processing complete!")
                
                # Display final results
                st.success("üéâ GitHub Actions processing completed!")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.subheader("üìä Processing Summary")
                    st.metric("Total Chunks", total_chunks)
                    st.metric("Successfully Processed", len(chunk_results))
                    st.metric("Fields Extracted", len(final_data))
                
                with col2:
                    st.subheader("üì• Download Results")
                    json_str = json.dumps(final_data, indent=2)
                    st.download_button(
                        label="Download GitHub Actions JSON",
                        data=json_str,
                        file_name="github_actions_structured.json",
                        mime="application/json"
                    )
                
                # Display final structured data
                st.subheader("üóÇÔ∏è Complete Structured GitHub Actions")
                st.json(final_data)
                
                # Individual chunk results
                st.subheader("üìë Individual Chunk Results")
                for chunk_name, data in chunk_results.items():
                    with st.expander(f"View {chunk_name}"):
                        st.json(data)
            
            else:
                st.error("Failed to extract text from Markdown")

elif page == "üìö Citations Parser":
    st.title("üìö Citations Parser")
    st.write("Upload a BibTeX file and get structured citation data using GPT-4 with chunked processing")
    
    # File uploader
    uploaded_file = st.file_uploader("Choose a BibTeX file (.bib)", type=["bib", "bibtex"])
    
    if uploaded_file is not None:
        st.success("BibTeX file uploaded successfully!")
        
        # Extract text button
        if st.button("Extract and Process Citations"):
            # Initialize progress tracking
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # Step 1: Extract text from BibTeX
            status_text.text("üìñ Extracting text from BibTeX...")
            document_text = extract_text_from_bibtex(uploaded_file)
            
            if document_text:
                st.success("‚úÖ Text extracted successfully!")
                
                # Show extracted text preview
                with st.expander("View Extracted Text Preview"):
                    st.text_area("Extracted Text", document_text[:1000] + "..." if len(document_text) > 1000 else document_text, height=200)
                
                # Step 2: Get chunk files
                status_text.text("üîç Finding citations chunk files...")
                chunk_files = get_citations_chunk_files()
                
                if not chunk_files:
                    st.error("No citations chunk files found!")
                    st.stop()
                
                st.info(f"Found {len(chunk_files)} chunk files")
                
                # Step 3: Initialize OpenAI client
                status_text.text("ü§ñ Initializing OpenAI client...")
                client = get_openai_client()
                
                # Step 4: Process each chunk
                chunk_results = {}
                total_chunks = len(chunk_files)
                
                for i, chunk_file in enumerate(chunk_files):
                    chunk_name = Path(chunk_file).stem
                    
                    status_text.text(f"‚ö° Processing {chunk_name} ({i+1}/{total_chunks})...")
                    progress_bar.progress((i + 1) / total_chunks)
                    
                    # Load response format
                    response_format = load_response_format(chunk_file)
                    if not response_format:
                        st.warning(f"Skipping {chunk_file} due to loading error")
                        continue
                    
                    # Create prompt for this chunk
                    prompt = f"""
                    Extract citation information from this BibTeX data for {chunk_name}:
                    
                    {document_text}
                    
                    Please extract the relevant citation information and return it in the specified JSON format.
                    """
                    
                    # Call GPT with retries
                    result = call_gpt_with_structured_output(client, prompt, response_format, 3)
                    
                    if result:
                        chunk_results[chunk_name] = result
                        st.success(f"‚úÖ Completed {chunk_name}")
                        
                        # Show preview of extracted data
                        with st.expander(f"Preview: {chunk_name}"):
                            st.json(result)
                    else:
                        st.warning(f"‚ö†Ô∏è Failed to process {chunk_name}")
                    
                    # Delay to avoid rate limiting
                    if i < total_chunks - 1:
                        time.sleep(0.5)
                
                # Step 5: Combine results
                status_text.text("üîó Combining all results...")
                final_data = combine_data(chunk_results)
                
                progress_bar.progress(1.0)
                status_text.text("‚úÖ Processing complete!")
                
                # Display final results
                st.success("üéâ Citations processing completed!")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.subheader("üìä Processing Summary")
                    st.metric("Total Chunks", total_chunks)
                    st.metric("Successfully Processed", len(chunk_results))
                    st.metric("Fields Extracted", len(final_data))
                
                with col2:
                    st.subheader("üì• Download Results")
                    json_str = json.dumps(final_data, indent=2)
                    st.download_button(
                        label="Download Citations JSON",
                        data=json_str,
                        file_name="citations_structured.json",
                        mime="application/json"
                    )
                
                # Display final structured data
                st.subheader("üóÇÔ∏è Complete Structured Citations")
                st.json(final_data)
                
                # Individual chunk results
                st.subheader("üìë Individual Chunk Results")
                for chunk_name, data in chunk_results.items():
                    with st.expander(f"View {chunk_name}"):
                        st.json(data)
            
            else:
                st.error("Failed to extract text from BibTeX")

# Footer
st.sidebar.markdown("---")
st.sidebar.markdown("### About")
st.sidebar.info("""
**Intelligent Chunked Processing Suite**

Solves the challenge of complex structured output by breaking documents into logical chunks, processing iteratively with GPT-4, and recombining with schema validation.

**Supports**: PDF Resumes ‚Ä¢ GitHub Actions (Markdown) ‚Ä¢ Citations (BibTeX)

Built with Streamlit + OpenAI Structured Output
""")
